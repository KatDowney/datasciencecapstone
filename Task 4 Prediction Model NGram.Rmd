---
title: "Task4"
author: "Kat Downey"
date: "15/08/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
install.packages("ngram", repos = "http://cran.us.r-project.org")
install.packages("tm", repos = "http://cran.us.r-project.org")
install.packages("RWeka", repos = "http://cran.us.r-project.org")
install.packages("knitr", repos = "http://cran.us.r-project.org")
install.packages("RColorBrewer", repos = "http://cran.us.r-project.org")
install.packages("stringi", repos = "http://cran.us.r-project.org")
install.packages("wordcloud2", repos = "http://cran.us.r-project.org")
install.packages("ggplot2", repos = "http://cran.us.r-project.org")
install.packages("NLP", repos = "http://cran.us.r-project.org")
install.packages("slam", repos = "http://cran.us.r-project.org")
install.packages("xtable", repos = "http://cran.us.r-project.org")
install.packages("dplyr", repos = "http://cran.us.r-project.org")
install.packages("stringr", repos = "http://cran.us.r-project.org")
install.packages("tidytext", repos = "http://cran.us.r-project.org")
install.packages("tidyr", repos = "http://cran.us.r-project.org")
library(ngram)
library(tm)
library(RWeka)
library(knitr)
library(RColorBrewer)
library(stringi)
library(wordcloud2)
library(ggplot2)
library(NLP)
library(slam)
library(xtable)
library(dplyr)
library(stringr)
library(tidytext)
library(tidyr)
```

**Load Data**

```{r prediction mode}

#reading in the data and turning it into a dataframe

blogs_file   <- "./final/en_US/en_US.blogs.txt"
news_file    <- "./final/en_US/en_US.news.txt"
twitter_file <- "./final/en_US/en_US.twitter.txt"  

blogs   <- readLines(blogs_file, skipNul = TRUE)
news    <- readLines(news_file,  skipNul = TRUE)
twitter <- readLines(twitter_file, skipNul = TRUE)

blogs   <- data_frame(text= blogs)
news    <- data_frame(text= news)
twitter <- data_frame(text= twitter)

```

**Sample Data**
Set the seed so those who want to replicate this will get the same results
#I'm also going to take a sample of the data as it's large (5%).

nrow is used to return the number of rows from a specific data set, here I'm using the function sample_n to select a random sample, saying subset the sample to this random sample, the data is from blogs and I want the specific 5% that i've detailed in sample_pct.

```{r sample}

#set the seed so those who want to replicate this will get the same results
#I'm also going to take a sample of the data as it's large (5%)

#nrow is used to return the number of rows from a specific data set, here I'm using the function 
set.seed(1001)
sample_pct <- 0.05

blogs_sample <- blogs %>%
    sample_n(., nrow(blogs)*sample_pct)

news_sample <- news %>%
    sample_n(., nrow(news)*sample_pct)

twitter_sample <- twitter %>%
    sample_n(., nrow(twitter)*sample_pct)

```


**Create Tidy dataset**

Next will create a combined sample - first using bind_rows to comine blogs,news and twitter samples and adding a source column.

Then the source is  numeric so change this to a factor variable

```{r tidy}

repo_sample <- bind_rows(mutate(blogs_sample, source = "blogs"),
                         mutate(news_sample, source = "news"),
                         mutate(twitter_sample, source = "twitter"))

repo_sample$source <- as.factor(repo_sample$source)
                        

```
**Clean**
Then I wanted to clean out my workspace so removed the below columns
```{r tidy2}

rm(list = c("blogs", "blogs_file", "blogs_sample","news", "news_file",     
            "news_sample", "sample_pct", "twitter","twitter_file", 
            "twitter_sample"))
                        
```

**Clean up sample data**

Here i am cleaning up the date to remove any spaces, any website address etc.
I'm also using inconv to convert the character vectors.
```{r tidy3}

replace_reg <- "[^[:alpha:][:space:]]*"
replace_url <- "http[^[:space:]]*"
replace_aaa <- "\\b(?=\\w*(\\w)\\1)\\w+\\b" 


#cleaning sample, cleaning is separated from tidying so unnest_tokens function can be used for words, and ngrams


clean_sample <-  repo_sample %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  mutate(text = str_replace_all(text, replace_url, "")) %>%
  mutate(text = str_replace_all(text, replace_aaa, "")) %>% 
  mutate(text = iconv(text, "ASCII//TRANSLIT"))

rm(list = c("repo_sample"))

```

**Create n-Grams**
I'm now creating my n-grams - n-grams is a contigous sequence of n items from a given sample of text or speech.


To create the n-grams i will be using the unnest_tokens function which will split the words out from the text column.

I've done this here for one word, two word repetitions etc
```{r ngrams}

#unigrams

tidy_repo <- clean_sample %>%
  unnest_tokens(word, text)

bigram_repo <- clean_sample %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
  
trigram_repo <- clean_sample %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)

quadgram_repo <- clean_sample %>%
  unnest_tokens(quadgram, text, token = "ngrams", n = 4)

quintgram_repo <- clean_sample %>%
  unnest_tokens(quintgram, text, token = "ngrams", n = 5)

sextgram_repo <- clean_sample  %>%
  unnest_tokens(sextgram, text, token = "ngrams", n = 6)


```

**Reduce size of ngram files**
These file sizes take a long time to run so I want to reduce the file size so I am going to cut these files by 50%.

First I count how many words I have
Then I add an column - Proportion which looks at the number of observations in the current group, the divided that by the sum of words.

The arrange the proportions from lowest to highest.

The used the coverage function which counts for each integer how many ranges overlap, then i look at the cumulative summary of the proportion which i had calculated above.

I then filter this so it only takes 50% of the sample.



```{r reduce_file_size}

cover_50 <- tidy_repo %>%
  count(word) %>%  
  mutate(proportion = n / sum(n)) %>%
  arrange(desc(proportion)) %>%  
  mutate(coverage = cumsum(proportion)) %>%
  filter(coverage <= 0.5)


bigram_cover_50 <- bigram_repo %>%
  count(bigram) %>%  
  mutate(proportion = n / sum(n)) %>%
  arrange(desc(proportion)) %>%  
  mutate(coverage = cumsum(proportion)) %>%
  filter(coverage <= 0.5)


trigram_cover_50 <- trigram_repo %>%
  count(trigram) %>%  
  mutate(proportion = n / sum(n)) %>%
  arrange(desc(proportion)) %>%  
  mutate(coverage = cumsum(proportion)) %>%
  filter(coverage <= 0.5)


quadgram_cover_50 <- quadgram_repo %>%
  count(quadgram) %>%  
  mutate(proportion = n / sum(n)) %>%
  arrange(desc(proportion)) %>%  
  mutate(coverage = cumsum(proportion)) %>%
  filter(coverage <= 0.5)
```
**Separate Words**
Now I need to separate the words out within the two/three/four repetitions.

Did this using the separate function


```{r separate_words}

bi_words <- bigram_cover_50 %>%
  separate(bigram, c("word1", "word2"), sep = " ")
bi_words

tri_words <- trigram_cover_50 %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")
tri_words

quad_words <- quadgram_cover_50 %>%
  separate(quadgram, c("word1", "word2", "word3", "word4"), sep = " ")
quad_words


```


**Save the files to use in the shiny app**

```{r save}

install.packages("datasets")
library(datasets)
saveRDS(bi_words, ".bi_words")
saveRDS(tri_words, ".tri_words")
saveRDS(quad_words,".quad_words")

```

**Clear workspace, Time loads**
```{r clear space}

go <- Sys.time()

stop <- Sys.time()
(how_long <- stop - go)

```

**Distribution of ngrams**
Use ggplot to create a histogram plot of distribution for the two/three and four word repetitons. 

Needed to use Rep to replicate the values and create a data.frame.
```{r distribution}
dist = data_frame(ngram = c(rep("bigrams",   nrow(bigram_cover_50)),
                             rep("trigrams",  nrow(trigram_cover_50)),
                             rep("quadgrams", nrow(quadgram_cover_50))), 
                   number = c(bigram_cover_50$n, trigram_cover_50$n, quadgram_cover_50$n))

dist$ngram <- as.factor(dist$ngram)
ggplot(data = dist, aes(y = number, x = ngram)) + geom_boxplot() + scale_y_log10()

```


**create ngram matching function**

Using the function input_words I will create bigrams, trigrams and quadgrams.

I'll filter each one to the specific bi_words/tri_words/quad_words, selecting the top Row and the amount of words I'm looking at, finally converting the output to a character.


```{r ngram match}
bigram <- function(input_words){
                    num <- length(input_words)
                    filter(bi_words, 
                          word1==input_words[num]) %>% 
                    top_n(1, n) %>%
                    filter(row_number() == 1L) %>%
                    select(num_range("word", 2)) %>%
                    as.character() -> out
                    ifelse(out =="character(0)", "?", return(out))
}

trigram <- function(input_words){
                    num <- length(input_words)
                    filter(tri_words, 
                            word1==input_words[num-1], 
                            word2==input_words[num])  %>% 
                    top_n(1, n) %>%
                    filter(row_number() == 1L) %>%
                    select(num_range("word", 3)) %>%
                    as.character() -> out
                    ifelse(out=="character(0)", bigram(input_words), return(out))
}

quadgram <- function(input_words){
                    num <- length(input_words)
                    filter(quad_words, 
                            word1==input_words[num-2], 
                            word2==input_words[num-1], 
                            word3==input_words[num])  %>% 
                    top_n(1, n) %>%
                    filter(row_number() == 1L) %>%
                    select(num_range("word", 4)) %>%
                    as.character() -> out
                    ifelse(out=="character(0)", trigram(input_words), return(out))
}

```

**Create User Input and Data Cleaning Function; Calls the matching functions**

The next bit creates function to output the next words for each bigram/trigram/quadgram.
```{R userinput}
ngrams <- function(input){
  # Create a dataframe
  input <- data_frame(text = input)
  # Clean the Inpput
  replace_reg <- "[^[:alpha:][:space:]]*"
  input <- input %>%
    mutate(text = str_replace_all(text, replace_reg, ""))
  # Find word count, separate words, lower case
  input_count <- str_count(input, boundary("word"))
  input_words <- unlist(str_split(input, boundary("word")))
  input_words <- tolower(input_words)
  # Call the matching functions
  out <- ifelse(input_count == 1, bigram(input_words), 
              ifelse (input_count == 2, trigram(input_words), quadgram(input_words)))
  # Output
  return(out)
}

```

**user input**

```{r userinput2}
input <- "I'd just like all of these questions answered, a presentation of evidence, and a jury to settle the"
ngrams(input)
```
